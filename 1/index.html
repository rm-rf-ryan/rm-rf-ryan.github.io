<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ryan Cheng CS180 Project 1</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>Project 1: Colorizing the Prokudin Gorskii Photo Collection</h1>
        <nav>
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../1/index.html">Project 1</a></li>
                <li><a href="../2/index.html">Project 2</a></li>
                <li><a href="../3/index.html">Project 3</a></li>
                <li><a href="../4/index.html">Project 4</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section>
            <h2>Overview</h2>
            <p>Sergei Mikhailovich Prokudin-Gorskii in 1907 anticipated the future of color photography by taking 3 photos per scene with a red, green, and blue filter over a black and white camera. These photos were bought by the Library of Congress, and this project aims to combine and colorize these photos as Gorskii envisioned.</p>
        </section>

        <section>
            <h2>Approach</h2>
            <p>I implemented the basic methods for aligning images (the naive approach of using trial and error to find offsets between images) as well as an image-pyramid approach for larger files that first scales down the image to a base case slightly larger than 32 by 32 pixels, find the offset vector through trial and error, and recursively iterate through larger versions of the image (multiplying the scale by 2 as well as the previous displacement vector) to finally arrive at a displacement for the largest image.</p>
            <p> For a metric for how well aligned the images are, I used the L2 norm between the images and also implemented Normalized Cross-Correlation (NCC) for images that do not work well with the L2 norm comparison.</p>
            <p> An extra feature I implemented that vastly improved the quality of the colorized image was using Canny edge detection to transform the images and align the images based off the transformed images. The Canny edge detector first applies a filter derived from a Gaussian function to smooth the image and reduce the influence of noise. Next, the intensity of the gradient at each pixel is calculated to find areas where intensity changes sharply. To refine the results, the algorithm trims down potential edges to single-pixel lines by discarding gradient magnitude values that are not local maxima. Lastly, it uses hysteresis thresholding to decide which remaining pixels truly form part of an edge based on their gradient strength. I expected NCC to work the best for these since they would compare edges directly, but actually the L2 norm turned out to work the best for the majority of photos in the dataset (probably because canny edge detection is a bit messy and sigma and the hysteresis thresholds would need to be finetuned for each image for NCC to work well). I've also implemented parallel threading in my code to speed up the image generation process from ~45 min for all photos to ~24 min for all photos.</p>
            <p>Images are separated by method, with a comparison of techniques at the bottom. The blue image was taken to be an offset of (0,0), and displacement vectors are listed as (vertical, horizontal) pointing down and to the right of the top left corner for positive displacements.</p>
            <p>Extra images I've chosen are marked with *asterisks*! </p>
        </section>
        <section>
            <h2>Naive L2 Norm with Edge Detection</h2>
            <p> Images are generated by trial and error of displacement vectors of RGB layers transformed by the Canny edge detector, comparing the L2 norm between them.</p>
            <div class="image-grid">
                <figure>
                    <figcaption><b>Cathedral</b></figcaption>
                    <img src="./media/cathedral.jpg" alt="cathedral.jpg">
                    <figcaption><b>g: </b>(5, 2)</figcaption>
                    <figcaption><b>r: </b>(12, 3) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Tobolsk</b></figcaption>
                    <img src="./media/tobolsk.jpg" alt="tobolsk.jpg">
                    <figcaption><b>g: </b>(3, 2)</figcaption>
                    <figcaption><b>r: </b>(6, 3) </figcaption>
                </figure>
            </div>
        </section>
        <section>
            <h2>Naive Normalized Cross-Correlation (NCC) with Edge Detection </h2>
            <p> Likewise trial and error of displacement vectors, but over layers transformed with Canny edge detection. Possibly due to the low resolution and light colors, monastery seems to work best only with NCC and edge detection as a comparison metric. </p>
            <div class="image-grid">
                <figure>
                    <figcaption><b>Monastery Naive (just L2 Norm)</b></figcaption>
                    <img src="./media/monasterynaive.jpg" alt="monasterynaive.jpg">
                    <figcaption><b>g: </b>(-6, 0)</figcaption>
                    <figcaption><b>r: </b>(9, 1) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Monastery</b></figcaption>
                    <img src="./media/monastery.jpg" alt="monastery.jpg">
                    <figcaption><b>g: </b>(-1, 3)</figcaption>
                    <figcaption><b>r: </b>(5, 4) </figcaption>
                </figure>
            </div>
        </section>
        <section>
            <h2>Image-Pyramid Edge Detection L2 Norm</h2>
            <p> Generated images are with an image-pyramid approach transformed using Canny edge detection with sigma=1.5 and sweeping over 50 pixel displacements per layer. Comparisons between layers are done using the L2 norm. Lady required sigma=0 to be aligned properly, possibly due to the soft colors with lack of contrast making the gradients smaller and below the hysteresis threshold. </p>

            <div class="image-grid">
                <figure>
                    <figcaption><b>Church</b></figcaption>
                    <img src="./media/church.jpg" alt="church.jpg">
                    <figcaption><b>g: </b>(25, 4)</figcaption>
                    <figcaption><b>r: </b>(58, -4) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Emir</b></figcaption>
                    <img src="./media/emir.jpg" alt="emir.jpg">
                    <figcaption><b>g: </b>(49, 23)</figcaption>
                    <figcaption><b>r: </b>(107, 40) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Harvesters</b></figcaption>
                    <img src="./media/harvesters.jpg" alt="harvesters.jpg">
                    <figcaption><b>g: </b>(60, 18)</figcaption>
                    <figcaption><b>r: </b>(123, 9) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Icon</b></figcaption>
                    <img src="./media/icon.jpg" alt="icon.jpg">
                    <figcaption><b>g: </b>(38, 16)</figcaption>
                    <figcaption><b>r: </b>(90, 22) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Onion Church</b></figcaption>
                    <img src="./media/onion_church.jpg" alt="onion_church.jpg">
                    <figcaption><b>g: </b>(52, 24)</figcaption>
                    <figcaption><b>r: </b>(107, 34) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Sculpture</b></figcaption>
                    <img src="./media/sculpture.jpg" alt="sculpture.jpg">
                    <figcaption><b>g: </b>(33, -11)</figcaption>
                    <figcaption><b>r: </b>(140, -27) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Self Portrait</b></figcaption>
                    <img src="./media/self_portrait.jpg" alt="self_portrait.jpg">
                    <figcaption><b>g: </b>(77, 29)</figcaption>
                    <figcaption><b>r: </b>(175, 37) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Three Generations</b></figcaption>
                    <img src="./media/three_generations.jpg" alt="three_generations.jpg">
                    <figcaption><b>g: </b>(56, 12)</figcaption>
                    <figcaption><b>r: </b>(111, 8) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Train</b></figcaption>
                    <img src="./media/train.jpg" alt="train.jpg">
                    <figcaption><b>g: </b>(49, 2)</figcaption>
                    <figcaption><b>r: </b>(84, 28) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Lady (sigma=0)</b></figcaption>
                    <img src="./media/lady.jpg" alt="lady.jpg">
                    <figcaption><b>g: </b>(57, 9)</figcaption>
                    <figcaption><b>r: </b>(120, 13) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>*Lugano*</b></figcaption>
                    <img src="./media/lugano.jpg" alt="lugano.jpg">
                    <figcaption><b>g: </b>(41, -17)</figcaption>
                    <figcaption><b>r: </b>(93, -29) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>*Zlatoust*</b></figcaption>
                    <img src="./media/zlatoust.jpg" alt="zlatoust.jpg">
                    <figcaption><b>g: </b>(37, -10)</figcaption>
                    <figcaption><b>r: </b>(140, -20) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>*The Dog In Me*</b></figcaption>
                    <img src="./media/dog.jpg" alt="dog.jpg">
                    <figcaption><b>g: </b>(61, 5)</figcaption>
                    <figcaption><b>r: </b>(137, 7) </figcaption>
                </figure>
            </div>
        </section>
        <section>
            <h2>Image-Pyramid L2 Norm</h2>
            <p> Image-pyramid approach sweeping over 50 pixel displacements per layer. Comparisons between layers are done using the L2 norm. Edge detections seems to have a hard time with aligning the melon image precisely (although it is within several pixels). After inspecting the Canny transformed images, I believe this is because there is a lot more blue contrast in the image to determine edges, and not as much green and red contrast. This makes it slightly more difficult to align green and red images precisely onto the blue. </p>
            <div class="image-grid">
                <figure>
                    <figcaption><b>Melons (Edge Detection)</b></figcaption>
                    <img src="./media/melonsedge.jpg" alt="melonsedge.jpg">
                    <figcaption><b>g: </b>(83, 10)</figcaption>
                    <figcaption><b>r: </b>(163, 11) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Melons</b></figcaption>
                    <img src="./media/melons.jpg" alt="melons.jpg">
                    <figcaption><b>g: </b>(83, 4)</figcaption>
                    <figcaption><b>r: </b>(176, 7) </figcaption>
                </figure>
            </div>
        </section>
        <section>
            <h2>Edge Detection vs. No Edge Detection</h2>
            <p> Images transformed with Canny Edge Detection generally do a lot better on a couple of the images than the non-edge-detection counterparts, and around equal on others. Below are some examples. </p>
            <div class="image-grid">
                <figure>
                    <figcaption><b>Emir (None)</b></figcaption>
                    <img src="./media/emir2.jpg" alt="emir2.jpg">
                    <figcaption><b>g: </b>(-3, 7)</figcaption>
                    <figcaption><b>r: </b>(107, 17) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Emir (Edge Detection)</b></figcaption>
                    <img src="./media/emir.jpg" alt="emir.jpg">
                    <figcaption><b>g: </b>(49, 23)</figcaption>
                    <figcaption><b>r: </b>(107, 40) </figcaption>
                </figure>
            </div>
            <div class="image-grid">
                <figure>
                    <figcaption><b>Harvesters (None)</b></figcaption>
                    <img src="./media/harvesters2.jpg" alt="harvesters2.jpg">
                    <figcaption><b>g: </b>(-118, -3)</figcaption>
                    <figcaption><b>r: </b>(120, 7) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Harvesters (Edge Detection)</b></figcaption>
                    <img src="./media/harvesters.jpg" alt="harvesters.jpg">
                    <figcaption><b>g: </b>(60, 18)</figcaption>
                    <figcaption><b>r: </b>(123, 9) </figcaption>
                </figure>
            </div>
            <div class="image-grid">
                <figure>
                    <figcaption><b>Self Portrait (None)</b></figcaption>
                    <img src="./media/self_portrait2.jpg" alt="self_portrait2.jpg">
                    <figcaption><b>g: </b>(50, -2)</figcaption>
                    <figcaption><b>r: </b>(130, -5) </figcaption>
                </figure>
                <figure>
                    <figcaption><b>Self Portrait (Edge Detection)</b></figcaption>
                    <img src="./media/self_portrait.jpg" alt="self_portrait.jpg">
                    <figcaption><b>g: </b>(77, 29)</figcaption>
                    <figcaption><b>r: </b>(175, 37) </figcaption>
                </figure>
            </div>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 <a href="https://github.com/<your_github_username>">My GitHub</a></p>
    </footer>
</body>
</html>