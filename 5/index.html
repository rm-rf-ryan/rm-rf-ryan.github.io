<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ryan Cheng CS180 Project 5</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>Project 5: Fun With Diffusion Models!</h1>
        <nav>
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../1/index.html">Project 1</a></li>
                <li><a href="../2/index.html">Project 2</a></li>
                <li><a href="../3/index.html">Project 3</a></li>
                <li><a href="../4/index.html">Project 4</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section>
            <h1>Part A: The Power of Diffusion Models!</h1>
            <h2>Overview</h2>
            <p> In the first part of this project I create interesting images and optical illusions using diffusion models, implementing variations of diffusion sampling loops. </p>
        </section>
        <section>
            <h2>Part 0: Setup</h2>
            <p> We use DeepFloyd IF for all the images generated in this part of the project, accessed via Hugging Face. It consists of 2 stages, the first of which produces images of size 64 x 64, and the second which takes those outputs and generates images of size 256 x 256. </p>
            <p>I upsample all of the generated images in the second half of part A to 256 x 256 if they are not already 256 x 256, and use a random seed of <b>180</b> for all of the proceeding generations. </p>
            <p>Below are the generated images for 3 prompts to the model with num_inference_steps = 5 and num_inference_steps = 20 for both stages. </p>
            <h3 style="text-align: center">num_inference_steps=5</h3>
            <div class="image-grid">
                <figure>
                    <figcaption><b>an oil painting of a snowy mountain village</b></figcaption>
                    <img src="./media/oil5.png" alt="chocolate.jpg">
                </figure>
    
                <figure>
                    <figcaption><b>a man wearing a hat</b></figcaption>
                    <img src="./media/man5.png" alt="chocolate_rect.png">
                </figure>
                <figure>
                    <figcaption><b>a rocket ship</b></figcaption>
                    <img src="./media/rocket5.png" alt="chocolate_rect.png">
                </figure>
            </div>
            <h3 style="text-align: center">num_inference_steps=20</h3>
            <div class="image-grid">
                
                <figure>
                    <figcaption><b>an oil painting of a snowy mountain village</b></figcaption>
                    <img src="./media/oil20.png" alt="chocolate.jpg">
                </figure>
    
                <figure>
                    <figcaption><b>a man wearing a hat</b></figcaption>
                    <img src="./media/man20.png" alt="chocolate_rect.png">
                </figure>
                <figure>
                    <figcaption><b>a rocket ship</b></figcaption>
                    <img src="./media/rocket20.png" alt="chocolate_rect.png">
                </figure>
            </div>
            <p>The images match the text prompts pretty well, even for a small number of inference steps, however the quality of the output varies based on the number of inference steps. There are visible noise artifacts from the generations with a small number of inference steps due to the upscaling (second stage) not being run for enough iterations. In addition, the images generated do not look very realistic (the proportions of the man's face are off as well as the number of fingers on his hand, as well as the rocket's flames being inconsistently drawn) which is probably due to the first stage not being run for enough inference steps. Larger inference steps seem to fix this, however the resulting images seem more cartoonish. </p>
         </section>

         <section>
            <h2>Part 1: Sampling Loops</h2>
            <p> We can start with a clean image x0 and iteratively add noise sampled from a Gaussian distribution for each timestep t until we end up with pure noise at timestep T. A diffusion model learns to reverse this process, given a noisy xt and timestep t it predicts the noise in the image. With this we can estimate the initial x0 or remove part of the noise, giving us xt-1. For DeepFloyd models, T=1000, and there are particular noise coefficients alpha bar t that dictate how we should add noise. </p>
            <h3>1.1 Implementing the Forward Process</h3>
            <img src="./media/forward.png" alt="forward_op" width="300" class="">
            <p>In the forward process, we add noise to an image following the above equation for t from 0 to 999. This gives us progressively more noisy images, as shown below (not upscaled). </p>
            <div class="image-grid1">
                <figure>
                    <figcaption><b>Berkeley Campanile (original)</b></figcaption>
                    <img src="./media/campanile.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>t=250</b></figcaption>
                    <img src="./media/campanile250.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>t=500</b></figcaption>
                    <img src="./media/campanile500.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>t=750</b></figcaption>
                    <img src="./media/campanile750.png" alt="chocolate.jpg">
                </figure>
            </div>
         </section>

         <section>
            <h3>1.2 Classical Denoising</h3>
            <p>We can attempt to denoise the image by convolving it with the Gaussian kernel, effectively blurring it. This works somewhat, but does not look very nice. I use a kernel size of 7 and sigma of 2 for the Gaussian kernel. </p>
            <div class="image-grid">
                <figure>
                    <figcaption><b>Noisy t=250</b></figcaption>
                    <img src="./media/campanile250.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Noisy t=500</b></figcaption>
                    <img src="./media/campanile500.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Noisy t=750</b></figcaption>
                    <img src="./media/campanile750.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid">
                <figure>
                    <img src="./media/campanileblur250.png" alt="chocolate.jpg">
                    <figcaption><b>Blurred t=250</b></figcaption>
                </figure>
                <figure>
                    <img src="./media/campanileblur500.png" alt="chocolate.jpg">
                    <figcaption><b>Blurred t=500</b></figcaption>
                </figure>
                <figure>
                    <img src="./media/campanileblur750.png" alt="chocolate.jpg">
                    <figcaption><b>Blurred t=750</b></figcaption>
                </figure>
            </div>
        </section>
        <section>
            <h3>1.3 Implementing One-Step Denoising</h3>
            <p>We can use the pretrained DeepFloyd model's stage 1 denoiser to predict and remove the noise added to the image in a single step. We can solve the equation in the previous section for x0 and use this to scale and subtract the estimated noise: </p>
            <img src="./media/denoise.png" alt="forward_op" width=200 class="">
            <p>The resulting images are displayed below for t=250, 500, and 750:</p>
            <div class="image-grid1">
                <figure>
                    <figcaption><b>Berkeley Campanile (original)</b></figcaption>
                    <img src="./media/campanile.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Noisy t=250</b></figcaption>
                    <img src="./media/campanile250.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Noisy t=500</b></figcaption>
                    <img src="./media/campanile500.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Noisy t=750</b></figcaption>
                    <img src="./media/campanile750.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid1">
                <figure>
                    <img src="./media/campanile.png" alt="chocolate.jpg">
                    <figcaption><b>Original (unchanged)</b></figcaption>
                </figure>
                <figure>
                    <img src="./media/campaniledenoise250.png" alt="chocolate.jpg">
                    <figcaption><b>Denoised t=250</b></figcaption>
                </figure>
                <figure>
                    <img src="./media/campaniledenoise500.png" alt="chocolate.jpg">
                    <figcaption><b>Denoised t=500</b></figcaption>
                </figure>
                <figure>
                    <img src="./media/campaniledenoise750.png" alt="chocolate.jpg">
                    <figcaption><b>Denoised t=750</b></figcaption>
                </figure>
            </div>
            <p>We can see that the denoising UNet does a lot better of a job of projecting the image onto the manifold of natural images, although more of the details are hallucinated the more noisy the original image is. </p>
        </section>
        <section>
            <h3>1.4 Implementing Iterative Denoising</h3>
            <p>Diffusion models work better when denoised iteratively. We can skip some steps to speed up inference, stepping from T=990 to T=0 in steps of 30. The update rule taking the stride into account is below:</p>
            <img src="./media/iterative.png" width=300 alt="chocolate.jpg">
            <p>t' is the next timestep (less than the current timestep t), alpha t = alpha bar t / alpha bar t', beta = 1 - alpha t, and x0 is the current estimate of the clean image using the equation for x0 in section 1.3. v sigma is the random noise DeepFloyd predicted. </p>
            <p>The results are displayed below with noise from i_start = 10 and timestep[10] and compared to the previous 2 methods we tried above. </p>
            <div class="image-grid4">
                <figure>
                    <figcaption><b>Noisy Campanile at t=690</b></figcaption>
                    <img src="./media/iter690.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Noisy Campanile at t=540</b></figcaption>
                    <img src="./media/iter540.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Noisy Campanile at t=390</b></figcaption>
                    <img src="./media/iter390.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Noisy Campanile at t=240</b></figcaption>
                    <img src="./media/iter240.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Noisy Campanile at t=90</b></figcaption>
                    <img src="./media/iter90.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid1">
                <figure>
                    <img src="./media/campanile.png" alt="chocolate.jpg">
                    <figcaption><b>Original</b></figcaption>
                </figure>
                <figure>
                    <img src="./media/iter.png" alt="chocolate.jpg">
                    <figcaption><b>Iterative Denoising</b></figcaption>
                </figure>
                <figure>
                    <img src="./media/onestep.png" alt="chocolate.jpg">
                    <figcaption><b>One-Step Denoising</b></figcaption>
                </figure>
                <figure>
                    <img src="./media/blur.png" alt="chocolate.jpg">
                    <figcaption><b>Gaussian Blurred</b></figcaption>
                </figure>
            </div>
            <p>We can see that the iterative generation produces a higher quality image with more details, however most of these details are hallucinated. </p>
        </section>
        <section>
            <h3>1.5 Diffusion Model Sampling</h3>
            <p>We can also repeat the iterative process in 1.4 and denoise images from randomly sampled noise and i_start=0, effectively projecting the noisy images onto the manifold of images learned by the model. We use a "null" prompt that doesn't have any specific meaning, "a high quality photo", in the following generations from random noise (upscaled): </p>
            <div class="image-grid4">
                <figure>
                    <figcaption><b>Sample 1</b></figcaption>
                    <img src="./media/diffusion1.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Sample 2</b></figcaption>
                    <img src="./media/diffusion2.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Sample 3</b></figcaption>
                    <img src="./media/diffusion3.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Sample 4</b></figcaption>
                    <img src="./media/diffusion4.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Sample 5</b></figcaption>
                    <img src="./media/diffusion5.png" alt="chocolate.jpg">
                </figure>
            </div>
        </section>
        <section>
            <h3>1.6 Classifier-Free Guidance (CFG)</h3>
            <p>The generated images are somewhat nonsensical. To improve image quality at the expense of image diversity, we implement Classifier-Free Guidance. We compute a conditional and unconditional noise estimate epsilon c and epsilon u, and set our new noise estimate to be:</p>
            <img src="./media/epsilon_combined.png" width=150 alt="chocolate.jpg">
            <p>Where gamma is a scaling factor. We set gamma = 7 for the following generations, and use a nuill prompt of "" for the unconditional guidance noise estimate. The upscaled results are displayed below:</p>
            <div class="image-grid4">
                <figure>
                    <figcaption><b>CFG Sample 1</b></figcaption>
                    <img src="./media/cfg1.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>CFG Sample 2</b></figcaption>
                    <img src="./media/cfg2.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>CFG Sample 3</b></figcaption>
                    <img src="./media/cfg3.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>CFG Sample 4</b></figcaption>
                    <img src="./media/cfg4.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>CFG Sample 5</b></figcaption>
                    <img src="./media/cfg5.png" alt="chocolate.jpg">
                </figure>
            </div>
        </section>
        <section>
            <h3>1.7 Image-to-image Translation</h3>
            <p>We can make edits to an existing image by taking an image, noising it, and forcing it back to the image manifold without conditioning, following the SDEdit algorithm. We run forward with starting indices of [1,3,5,7,10,20] and apply it to the Campanile image, as well as 2 pictures I've taken (one of Osaka castle and another of me and my friends), displaying the upsampled results below (original images are shown at 256x256 resolution for comparison but they are inputted to the model as 64x64): </p>
            <div class="image-grid5">
                <figure>
                    <figcaption><b>i = 1</b></figcaption>
                    <img src="./media/campaniletrans1.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 3</b></figcaption>
                    <img src="./media/campaniletrans3.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 5</b></figcaption>
                    <img src="./media/campaniletrans5.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 7</b></figcaption>
                    <img src="./media/campaniletrans7.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 10</b></figcaption>
                    <img src="./media/campaniletrans10.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 20</b></figcaption>
                    <img src="./media/campaniletrans20.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Original</b></figcaption>
                    <img src="./media/campanilelarge.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid5">
                <figure>
                    <img src="./media/castletrans1.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/castletrans3.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/castletrans5.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/castletrans7.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/castletrans10.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/castletrans20.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/osaka.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid5">
                <figure>
                    <img src="./media/ryantrans1.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/ryantrans3.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/ryantrans5.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/ryantrans7.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/ryantrans10.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/ryantrans20.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/ryan.png" alt="chocolate.jpg">
                </figure>
            </div>
        </section>
        <section>
            <h4>1.7.1 Editing Hand-Drawn and Web Images</h4>
            <p>We can use the above procedure to project nonrealistic images onto the natural image manifold. Displayed below are 1 result from the Internet (Osaka from Azumanga Daioh) as well as 2 of my own sketches. </p>
            <div class="image-grid5">
                <figure>
                    <figcaption><b>i = 1</b></figcaption>
                    <img src="./media/osaka1.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 3</b></figcaption>
                    <img src="./media/osaka3.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 5</b></figcaption>
                    <img src="./media/osaka5.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 7</b></figcaption>
                    <img src="./media/osaka7.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 10</b></figcaption>
                    <img src="./media/osaka10.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 20</b></figcaption>
                    <img src="./media/osaka20.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Original</b></figcaption>
                    <img src="./media/osaka_anime.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid5">
                <figure>
                    <img src="./media/cat1.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/cat3.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/cat5.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/cat7.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/cat10.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/cat20.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/cat.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid5">
                <figure>
                    <img src="./media/boat1.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/boat3.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/boat5.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/boat7.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/boat10.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/boat20.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/drawnisland.png" alt="chocolate.jpg">
                </figure>
            </div>
        </section>
        <section>
            <h4>1.7.2 Inpainting</h4>
            <p>We can direct where the model should hallucinate new information by using a binary mask with the original image. Given an image x_orig and a binary mask m, we create a new image with the same content where m is 0 and new content where m is 1. We use same diffusion denoising loop except updating xt with the following after every step:</p>
            <img src="./media/inpaint.png" width=300 alt="chocolate.jpg">
            <div class="image-grid1">
                <figure>
                    <figcaption><b>Original</b></figcaption>
                    <img src="./media/campanilelarge.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Mask</b></figcaption>
                    <img src="./media/campanile_mask.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Hole to Fill</b></figcaption>
                    <img src="./media/masked_campanile.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Inpainted & Upscaled</b></figcaption>
                    <img src="./media/campanile_inpaint.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid1">
                <figure>
                    <img src="./media/osaka.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/castle_mask.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/masked_castle.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/castle_inpaint.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid1">
                <figure>
                    <img src="./media/ryan.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/ryan_mask.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/masked_ryan.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/ryan_inpaint.png" alt="chocolate.jpg">
                </figure>
            </div>
            <p>Notably, upscaling ends up modifying features from the original image. In general, I believe that this is helpful in making a more cohesive image, however in some cases we do not want this to happen. In Part 2: Bells and Whistles I examine using a binary mask and our old friend Laplacian Blending with the 256x256 image and the upscaled image to keep details from the original image. </p>
        </section>
        <section>
            <h4>1.7.3 Text-Conditional Image-to-image Translation</h4>
            <p> We repeat the process in 1.7 and 1.7.1, perturbing the images with noise, except this time we guide it with a text prompt "a rocket ship" instead of "a high quality photo" to guide generation towards the image subspace of rockets.</p>
            <div class="image-grid5">
                <figure>
                    <figcaption><b>i = 1</b></figcaption>
                    <img src="./media/rocketcampanile1.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 3</b></figcaption>
                    <img src="./media/rocketcampanile3.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 5</b></figcaption>
                    <img src="./media/rocketcampanile5.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 7</b></figcaption>
                    <img src="./media/rocketcampanile7.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 10</b></figcaption>
                    <img src="./media/rocketcampanile10.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>i = 20</b></figcaption>
                    <img src="./media/rocketcampanile20.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Original</b></figcaption>
                    <img src="./media/campanilelarge.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid5">
                <figure>
                    <img src="./media/rocketcastle1.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/rocketcastle3.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/rocketcastle5.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/rocketcastle7.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/rocketcastle10.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/rocketcastle20.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/osaka.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid5">
                <figure>
                    <img src="./media/rocketryan1.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/rocketryan3.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/rocketryan5.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/rocketryan7.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/rocketryan10.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/rocketryan20.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/ryan.png" alt="chocolate.jpg">
                </figure>
            </div>
        </section>
        
        <section>
            <h3>1.8 Visual Anagrams</h3>
            <p>We now proceed to make optical illusions using our diffusion models. In this section, we create an image that looks like one prompt but when flipped upside down looks like another prompt. In order to do this, we compute 2 separate noise estimates using the first prompt and the flipped image with the second prompt and average them t: </p>
            <img src="./media/visual_anagrams.png" width=200 alt="chocolate.jpg">
            <p>When we upscale the images, we use the "null" prompt "a high quality photo". </p>
            <div class="image-grid">
                <figure>
                    <figcaption><b>an oil painting of people around a campfire</b></figcaption>
                    <img src="./media/campfire.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>a photo of a hipster barista</b></figcaption>
                    <img src="./media/barista.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>an oil painting of a snowy mountain village</b></figcaption>
                    <img src="./media/village.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid">
                <figure>
                    <img src="./media/old_man.png" alt="chocolate.jpg">
                    <figcaption><b>an oil painting of an old man</b></figcaption>
                </figure>
                <figure>
                    <img src="./media/dog.png" alt="chocolate.jpg">
                    <figcaption><b>a photo of a dog</b></figcaption>
                </figure>
                <figure>
                    <img src="./media/waterfall.png" alt="chocolate.jpg">
                    <figcaption><b>a lithograph of waterfalls</b></figcaption>
                </figure>
            </div>
        </section>
        <section>
            <h3>1.10 Hybrid Images</h3>
            <p>We can create hybrid images in a similar process to 1.8, by combining noise estimates. This time, we perform a lowpass filter (convolve with gaussian) for one prompt's noise estimate, and a high pass (laplacian filter) of the second prompt's noise estimate: </p>
            <img src="./media/hybrid.png" width=200 alt="chocolate.jpg">
            <div class="image-grid">
                <figure>
                    <figcaption><b>Hybrid skull and waterfall</b></figcaption>
                    <img src="./media/skull_waterfall.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Hybrid dog and Amalfi Coast</b></figcaption>
                    <img src="./media/dog_coast.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Hybrid pencil and man in a hat</b></figcaption>
                    <img src="./media/man_pencil.png" alt="chocolate.jpg">
                </figure>
            </div>
        </section>
        <section>
            <h2>Part 2: Bells and Whistles</h2>
            <p>Recall in part 1.7.2, the upscaling of inpainted images results in details the original image also being hallucinated. I experiment with using multi-resolution blending to combine the upscaled in-painted image with the 256x256 original image to preserve the original image's details as opposed to simply masking the upscaled in-painted image over the original image. </p>
            <p>Using multi-resolution blending, we find marginal improvements over naive cropping, although the efficacy definitely depends on the images used. </p>
            <div class="image-grid1">
                <figure>
                    <figcaption><b>Original</b></figcaption>
                    <img src="./media/campanilelarge.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Inpainted & Upscaled</b></figcaption>
                    <img src="./media/campanile_inpaint.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Cropped onto Original</b></figcaption>
                    <img src="./media/campanile_inpaint_crop.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>Multi-Resolution Blending</b></figcaption>
                    <img src="./media/campanile_inpaint_blend.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid1">
                <figure>
                    <img src="./media/osaka.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/castle_inpaint.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/castle_inpaint_crop.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/castle_inpaint_blend.png" alt="chocolate.jpg">
                </figure>
            </div>
            <div class="image-grid1">
                <figure>
                    <img src="./media/ryan.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/ryan_inpaint.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/ryan_inpaint_crop.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <img src="./media/ryan_inpaint_blend.png" alt="chocolate.jpg">
                </figure>
            </div>
        </section>

        <section>
            <h1>Part B: Diffusion Models from Scratch!</h1>
            <h2>Overview</h2>
            <p>In the second part of this project we implement, train, and test the UNet architecture on denoising and generating images using the MNIST dataset. Time Conditioning and Class Conditioning with classifier-free guidance are incorporated into our model. </p>
        </section>
        <section>
            <h2>Part 1: Training a Single-Step Denoising UNet</h2>
            <p>We first optimize a simple one-step denoiser over the L2 loss between the model's prediction and the clean image. </p>
            <h3>1.1 Implementing the UNet</h3>
            <p>We implement the following architecture for an Unconditional UNet:</p>
            <img src="./media/uncondune5t.png" width=1000 alt="chocolate.jpg">
            <h3>1.2 Using the UNet to Train a Denoiser</h3>
            <p>For each training batch, we generate z=x + sigma * epsilon, where epsilon is sampled from N(0, I). We input z into our model, and train it using the L2 distance between the model's prediction for z and the original image x. </p>
            <p>We visualize the varying levels of noise on the MNIST digits below: </p>
            <img src="./media/mnist_noise.png" width=50% alt="chocolate.jpg" class="center">
            <h4>1.2.1 Training</h4>
            <p>We train the model to denoise noisy images with sigma=0.5, with batch size 256, 5 epochs, hidden dimension 128, and adam optimizer with learning rate 1e-4. </p>
            <p>The loss curve is plotted below:</p>
            <img src="./media/unconditioned_loss.png" width=50% alt="chocolate.jpg" class="center">
            <p>We visualize the denoised results on a small test sample batch for 1 epoch and 5 epochs:</p>
            <div class="image-grid2">
                <figure>
                    <figcaption><b>1 epoch</b></figcaption>
                    <img src="./media/1epochresult.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>5 epochs</b></figcaption>
                    <img src="./media/5epochresult.png" alt="chocolate.jpg">
                </figure>
            </div>
            <p>This UNet does a pretty good job at denoising the images at the noise level it is trained on. </p>
            <h4>1.2.2 Out-of-Distribution Testing</h4>
            <p>In this section, we see how well the denoiser does at different noise levels, for sigma in [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]. </p>
            <img src="./media/denoise_zero.png" width=50% alt="chocolate.jpg" class="center">
            <p>We can observe that the model generally does worse for noise levels it was not trained on, but this effect is most pronounced at higher noise levels as opposed to lower noise levels. </p>
        </section>
        <section>
            <h2>Part 2: Training a Diffusion Model</h2>
            <p>Instead of training with the L2 loss between our model's predicted image and the clean image, we instead iteratively denoise the image by training the model instead on the L2 loss between the model's prediction and the noise added to the image. </p>
            <p> We use the same formula as the forward process in part A to generate noisy images. Our betas are set to be evenly spaced between beta0=0.0001 and betaT=0.02 for beta between 1 and T-1, and the alphas and alpha bars are defined based on the betas as in part A. </p>
            <p>We use T=300 instead of T=1000 in part A as MNIST digits are relatively simple.</p>
            <h3>2.1 Adding Time Conditioning to UNet</h3>
            <p>We condition our UNet model with the time t that the model should be trained on by inserting FC blocks in the following manner, normalizing t before embedding it as t/T:</p>
            <img src="./media/time_conditioning_diagram.png" width=50% alt="chocolate.jpg" class="center">
            <h3>2.2 Training the UNet</h3>
            <p> We follow the following algorithm to train our time-conditioned UNet, sampling a random image from the training set, a random t, and training the denoiser to predict the noise in xt: </p>
            <img src="./media/time_conditioned_training.png" width=50% alt="chocolate.jpg" class="center">
            <p>The loss curve for this model is displayed below:</p>
            <img src="./media/time_conditioned_loss.png" width=50% alt="chocolate.jpg" class="center">
            <h3>2.3 Sampling from the UNet</h3>
            <p>We follow a similar sampling process to part A, except we don't predict the variance:</p>
            <img src="./media/time_conditioned_sampling.png" width=50% alt="chocolate.jpg" class="center">
            <p>Our sampling results for the time-conditioned UNet for 5 and 20 epochs are displayed below:</p>
            <div class="image-grid2">
                <figure>
                    <figcaption><b>5 epochs</b></figcaption>
                    <img src="./media/time5epochresult.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>20 epochs</b></figcaption>
                    <img src="./media/time20epochresult.png" alt="chocolate.jpg">
                </figure>
            </div>
            <p>We can see that the model does a bit poorly, generating some recognizable digits from random noise, however there are also a lot of random strokes and figures that do not look like digits.</p>
            <h3>2.4 Adding Class-Conditioning to UNet</h3>
            <p>We can get better results by also conditioning UNet on the class of the digit 0-9. This requires us to embed 2 more FCBlocks to the UNet (in the same locations that the blocks for t are placed, except multiplying the model outputs instead ofbeng added onto them). </p>
            <p>To make the model still work without being conditioned on the class, we also implement dropout, masking the class conditioning vector c to 0 for 10% of the training time. The class conditioned training algorithm is displayed below:</p>
            <img src="./media/algo3_c.png" width=50% alt="chocolate.jpg" class="center">
            <p>Our training loss curve for this model is as follows:</p>
            <img src="./media/class_conditioned_loss.png" width=50% alt="chocolate.jpg" class="center">
            <h3>2.5 Sampling from the Class-Conditioned UNet</h3>
            <p>The sampling process is the same as part A, and we use classifier-free guidance with gamma=5.0 to improve the conditional results. The algorithm is displayed below:</p>
            <img src="./media/algo4_c.png" width=50% alt="chocolate.jpg" class="center">
            <p>We visualize our results for all 10 digits below for epochs 5 and 20:</p>
            <div class="image-grid2">
                <figure>
                    <figcaption><b>5 epochs</b></figcaption>
                    <img src="./media/class5epochresult.png" alt="chocolate.jpg">
                </figure>
                <figure>
                    <figcaption><b>20 epochs</b></figcaption>
                    <img src="./media/class20epochresult.png" alt="chocolate.jpg">
                </figure>
            </div>
            <p>We can see that the results for both epochs are relatively good, but the 20 epoch model tends to keep more finer details. </p>
        </section>

    </main>

    <footer>
        <p>&copy; 2024 <a href="https://github.com/rm-rf-ryan">My GitHub</a></p>
    </footer>
</body>
</html>